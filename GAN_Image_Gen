{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Creating Realistic Images with GANs: A Deep Learning Challenge\n\nThis notebook implements a Generative Adversarial Network (GAN) to create realistic images. GANs consist of two neural networks that compete against each other:\n\n1. **Generator**: Creates fake images from random noise\n2. **Discriminator**: Tries to distinguish between real and fake images\n\nThrough this adversarial training process, the generator gradually learns to create more realistic images.","metadata":{}},{"cell_type":"markdown","source":"## 1. Import Required Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.datasets import fashion_mnist\n\n# Check if GPU is available\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:53:40.487760Z","iopub.execute_input":"2025-08-01T14:53:40.487976Z","iopub.status.idle":"2025-08-01T14:53:44.633790Z","shell.execute_reply.started":"2025-08-01T14:53:40.487947Z","shell.execute_reply":"2025-08-01T14:53:44.632904Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Load and Preprocess Dataset\n\nFor this example, we'll use the Fashion MNIST dataset, which contains 70,000 grayscale images of fashion items (clothing, shoes, bags, etc.) in 10 categories. Each image is 28x28 pixels.","metadata":{}},{"cell_type":"code","source":"# Load Fashion MNIST dataset\n(train_images, train_labels), (_, _) = fashion_mnist.load_data()\n\n# Preprocess the data\n# Reshape images to add channel dimension (28x28x1)\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n\n# Normalize pixel values to [-1, 1]\ntrain_images = (train_images - 127.5) / 127.5\n\n# Create TensorFlow dataset\nBUFFER_SIZE = 60000  # Size of the training dataset\nBATCH_SIZE = 256\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\n# Preview some images\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.imshow(train_images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n    plt.axis('off')\nplt.suptitle('Sample Images from Fashion MNIST Dataset', fontsize=16)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:53:44.640375Z","iopub.execute_input":"2025-08-01T14:53:44.640911Z","iopub.status.idle":"2025-08-01T14:53:47.619481Z","shell.execute_reply.started":"2025-08-01T14:53:44.640872Z","shell.execute_reply":"2025-08-01T14:53:47.618770Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Define Generator Model\n\nThe generator takes a random noise vector and transforms it into an image. It uses upsampling layers to increase the spatial dimensions from the initial noise vector to the final image size.","metadata":{}},{"cell_type":"code","source":"def make_generator_model():\n    \"\"\"Create the generator model to transform noise into images\"\"\"\n    \n    # Noise vector input dimension\n    NOISE_DIM = 100\n    \n    model = models.Sequential()\n    \n    # First layer - Dense layer to reshape noise\n    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(NOISE_DIM,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    # Reshape to start the convolutional layers\n    model.add(layers.Reshape((7, 7, 256)))\n    \n    # First upsampling block: 7x7 -> 14x14\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    # Second upsampling block: 14x14 -> 14x14\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    # Final upsampling block: 14x14 -> 28x28 with single channel (grayscale)\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    \n    return model\n\n# Create the generator\ngenerator = make_generator_model()\n\n# Test the generator with random noise\nNOISE_DIM = 100\nnoise = tf.random.normal([1, NOISE_DIM])\ngenerated_image = generator(noise, training=False)\n\n# Display a sample generated image\nplt.figure(figsize=(4, 4))\nplt.imshow(generated_image[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\nplt.axis('off')\nplt.title('Initial Random Generated Image')\nplt.show()\n\n# Display the generator architecture\ngenerator.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:53:47.620535Z","iopub.execute_input":"2025-08-01T14:53:47.620855Z","iopub.status.idle":"2025-08-01T14:53:48.999737Z","shell.execute_reply.started":"2025-08-01T14:53:47.620828Z","shell.execute_reply":"2025-08-01T14:53:48.999137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Define Discriminator Model\n\nThe discriminator is a binary classifier that tries to distinguish between real images from the dataset and fake images created by the generator.","metadata":{}},{"cell_type":"code","source":"def make_discriminator_model():\n    \"\"\"Create the discriminator model to classify images as real or fake\"\"\"\n    \n    model = models.Sequential()\n    \n    # First convolutional block\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', \n                           input_shape=[28, 28, 1]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    # Second convolutional block\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    # Flatten and output layer\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))  # No activation - we'll use sigmoid in the loss function\n    \n    return model\n\n# Create the discriminator\ndiscriminator = make_discriminator_model()\n\n# Test the discriminator with a real image and the previously generated image\ndecision_real = discriminator(tf.expand_dims(train_images[0], 0), training=False)\ndecision_fake = discriminator(generated_image, training=False)\n\nprint(f\"Discriminator output for real image: {decision_real.numpy()[0,0]}\")\nprint(f\"Discriminator output for fake image: {decision_fake.numpy()[0,0]}\")\n\n# Display the discriminator architecture\ndiscriminator.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:53:49.000632Z","iopub.execute_input":"2025-08-01T14:53:49.000817Z","iopub.status.idle":"2025-08-01T14:53:49.149152Z","shell.execute_reply.started":"2025-08-01T14:53:49.000802Z","shell.execute_reply":"2025-08-01T14:53:49.148496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Configure GAN Model\n\nNow we'll define the loss functions and optimizers for both the generator and discriminator networks.","metadata":{}},{"cell_type":"code","source":"# Define loss functions\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real_output, fake_output):\n    \"\"\"Calculate the discriminator loss\n    \n    The discriminator wants to maximize log(D(x)) + log(1 - D(G(z)))\n    Which is equivalent to minimizing the negative of that expression\n    \"\"\"\n    # Real images should be classified as 1\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    \n    # Fake images should be classified as 0\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    \n    # Total loss is the sum of both\n    total_loss = real_loss + fake_loss\n    \n    return total_loss\n\ndef generator_loss(fake_output):\n    \"\"\"Calculate the generator loss\n    \n    The generator wants to minimize log(1 - D(G(z)))\n    Which is equivalent to maximizing log(D(G(z)))\n    \"\"\"\n    # The generator wants the discriminator to classify fake images as 1\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\n# Define optimizers\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n\n# Define the noise dimension used for generating images\nNOISE_DIM = 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:53:49.150018Z","iopub.execute_input":"2025-08-01T14:53:49.150724Z","iopub.status.idle":"2025-08-01T14:53:49.164789Z","shell.execute_reply.started":"2025-08-01T14:53:49.150702Z","shell.execute_reply":"2025-08-01T14:53:49.163994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Create Training Loop\n\nHere we implement the adversarial training loop where the generator and discriminator train simultaneously. First, we'll define the training step and then the full training function.","metadata":{}},{"cell_type":"code","source":"# Create a directory to save generated images\nif not os.path.exists('generated_images'):\n    os.makedirs('generated_images')\n\n# Create a function to generate and save images\ndef generate_and_save_images(model, epoch, test_input, save_path=None):\n    \"\"\"Generate images from the model and optionally save them\"\"\"\n    # Generate images\n    predictions = model(test_input, training=False)\n    \n    # Plot the generated images\n    fig = plt.figure(figsize=(10, 10))\n    \n    for i in range(predictions.shape[0]):\n        plt.subplot(4, 4, i+1)\n        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n        plt.axis('off')\n    \n    plt.suptitle(f'Generated Images at Epoch {epoch}', fontsize=16)\n    plt.tight_layout()\n    \n    # Save if a path is provided\n    if save_path:\n        plt.savefig(save_path)\n    \n    plt.show()\n    plt.close()\n    \n    return predictions\n\n# Create a checkpoint manager to save model checkpoints\ncheckpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                discriminator_optimizer=discriminator_optimizer,\n                                generator=generator,\n                                discriminator=discriminator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:53:49.165609Z","iopub.execute_input":"2025-08-01T14:53:49.165884Z","iopub.status.idle":"2025-08-01T14:53:49.180681Z","shell.execute_reply.started":"2025-08-01T14:53:49.165859Z","shell.execute_reply":"2025-08-01T14:53:49.180030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the training step as a TensorFlow function for better performance\n@tf.function\ndef train_step(images):\n    \"\"\"Perform one training step on a batch of real images\"\"\"\n    # Generate random noise for the generator\n    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n    \n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        # Generate fake images\n        generated_images = generator(noise, training=True)\n        \n        # Get discriminator predictions for both real and fake images\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n        \n        # Calculate losses\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n        \n    # Calculate gradients\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    \n    # Apply gradients\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n    return gen_loss, disc_loss\n\n# Define the training function\ndef train(dataset, epochs, save_interval=10):\n    \"\"\"Train the GAN for the specified number of epochs\"\"\"\n    # Create fixed noise vectors for visualization\n    seed = tf.random.normal([16, NOISE_DIM])\n    \n    # Lists to track losses\n    gen_losses = []\n    disc_losses = []\n    \n    # Training loop\n    for epoch in range(epochs):\n        start = time.time()\n        \n        epoch_gen_loss = 0\n        epoch_disc_loss = 0\n        num_batches = 0\n        \n        # Train on each batch\n        for image_batch in dataset:\n            batch_gen_loss, batch_disc_loss = train_step(image_batch)\n            epoch_gen_loss += batch_gen_loss\n            epoch_disc_loss += batch_disc_loss\n            num_batches += 1\n        \n        # Calculate average losses for the epoch\n        epoch_gen_loss /= num_batches\n        epoch_disc_loss /= num_batches\n        \n        # Add to loss history\n        gen_losses.append(epoch_gen_loss)\n        disc_losses.append(epoch_disc_loss)\n        \n        # Print progress\n        print(f'Epoch {epoch+1}/{epochs}, Time: {time.time()-start:.2f}s')\n        print(f'Generator Loss: {epoch_gen_loss:.4f}, Discriminator Loss: {epoch_disc_loss:.4f}')\n        \n        # Save checkpoint every 15 epochs\n        if (epoch + 1) % 15 == 0:\n            checkpoint.save(file_prefix=checkpoint_prefix)\n        \n        # Generate and save images at regular intervals\n        if (epoch + 1) % save_interval == 0 or epoch == 0 or epoch == epochs - 1:\n            save_path = f'generated_images/epoch_{epoch+1:04d}.png'\n            generate_and_save_images(generator, epoch + 1, seed, save_path)\n    \n    # Return loss history for plotting\n    return np.array(gen_losses), np.array(disc_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:53:49.181603Z","iopub.execute_input":"2025-08-01T14:53:49.181882Z","iopub.status.idle":"2025-08-01T14:53:49.210451Z","shell.execute_reply.started":"2025-08-01T14:53:49.181858Z","shell.execute_reply":"2025-08-01T14:53:49.209826Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Train the GAN\n\nNow let's train the GAN model. This will take some time depending on your hardware. We'll save images at regular intervals to track the progress.","metadata":{}},{"cell_type":"code","source":"# Train the GAN\nEPOCHS = 50\nSAVE_INTERVAL = 5  # Save images every 5 epochs\n\n# Train the model and record the losses\ngenerator_losses, discriminator_losses = train(train_dataset, EPOCHS, SAVE_INTERVAL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:53:49.211411Z","iopub.execute_input":"2025-08-01T14:53:49.211725Z","iopub.status.idle":"2025-08-01T15:04:37.745029Z","shell.execute_reply.started":"2025-08-01T14:53:49.211699Z","shell.execute_reply":"2025-08-01T15:04:37.744434Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Visualize Training Progress\n\nNow let's visualize the training progress by plotting the loss curves and comparing generated images across epochs.","metadata":{}},{"cell_type":"code","source":"# Plot the loss curves\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(generator_losses)\nplt.title('Generator Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\nplt.subplot(1, 2, 2)\nplt.plot(discriminator_losses)\nplt.title('Discriminator Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\nplt.tight_layout()\nplt.savefig('generated_images/loss_curves.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T15:04:37.746132Z","iopub.execute_input":"2025-08-01T15:04:37.746772Z","iopub.status.idle":"2025-08-01T15:04:38.245862Z","shell.execute_reply.started":"2025-08-01T15:04:37.746750Z","shell.execute_reply":"2025-08-01T15:04:38.245035Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Generate Final Images\n\nGenerate a larger set of final images using our trained model.","metadata":{}},{"cell_type":"code","source":"# Generate a larger set of final images\nfinal_seed = tf.random.normal([25, NOISE_DIM])\nfinal_images = generator(final_seed, training=False)\n\n# Plot the images in a 5x5 grid\nplt.figure(figsize=(12, 12))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.imshow(final_images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n    plt.axis('off')\n    \nplt.suptitle('Final Generated Images', fontsize=20)\nplt.tight_layout()\nplt.savefig('generated_images/final_images.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T15:04:38.246629Z","iopub.execute_input":"2025-08-01T15:04:38.246887Z","iopub.status.idle":"2025-08-01T15:04:40.566696Z","shell.execute_reply.started":"2025-08-01T15:04:38.246848Z","shell.execute_reply":"2025-08-01T15:04:40.565872Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Create an Animation of the Generation Process","metadata":{}},{"cell_type":"code","source":"# Optional: Create an animation of the generation process\ntry:\n    from IPython.display import HTML\n    import matplotlib.animation as animation\n    import glob\n    \n    # Get all the saved images\n    image_files = sorted(glob.glob('generated_images/epoch_*.png'))\n    \n    if len(image_files) > 0:\n        # Create a figure for the animation\n        fig = plt.figure(figsize=(8, 8))\n        \n        # Function to update the figure for each frame\n        def update_fig(i):\n            img = plt.imread(image_files[i])\n            plt.imshow(img)\n            plt.axis('off')\n            plt.title(f'Epoch {i*SAVE_INTERVAL+1}')\n        \n        # Create the animation\n        anim = animation.FuncAnimation(fig, update_fig, frames=len(image_files), interval=500)\n        \n        # Save the animation as a gif\n        anim.save('generated_images/training_progress.gif', writer='pillow', fps=2)\n        \n        # Display the animation\n        plt.close()\n        HTML(anim.to_jshtml())\n    else:\n        print(\"No saved images found for animation.\")\nexcept Exception as e:\n    print(f\"Could not create animation: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T15:06:34.933176Z","iopub.execute_input":"2025-08-01T15:06:34.933808Z","iopub.status.idle":"2025-08-01T15:07:19.400745Z","shell.execute_reply.started":"2025-08-01T15:06:34.933786Z","shell.execute_reply":"2025-08-01T15:07:19.399940Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 11. Save the Trained Models","metadata":{}},{"cell_type":"code","source":"# Save the trained models\ngenerator.save('generated_images/generator_model.keras')\ndiscriminator.save('generated_images/discriminator_model.keras')\n\nprint(\"Models saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T15:07:19.402246Z","iopub.execute_input":"2025-08-01T15:07:19.402464Z","iopub.status.idle":"2025-08-01T15:07:19.487275Z","shell.execute_reply.started":"2025-08-01T15:07:19.402432Z","shell.execute_reply":"2025-08-01T15:07:19.486438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 12. Sample Images at Different Training Stages\n\nHere we'll load and display images from different stages of training to observe the evolution of the generator.","metadata":{}},{"cell_type":"code","source":"# Load and display images from early, middle, and late stages of training\ntry:\n    import glob\n    import matplotlib.image as mpimg\n    \n    # Get all image files\n    image_files = sorted(glob.glob('generated_images/epoch_*.png'))\n    \n    if len(image_files) >= 3:\n        # Select images from early, middle, and late stages\n        early_idx = 0\n        middle_idx = len(image_files) // 2\n        late_idx = len(image_files) - 1\n        \n        # Load the images\n        early_img = mpimg.imread(image_files[early_idx])\n        middle_img = mpimg.imread(image_files[middle_idx])\n        late_img = mpimg.imread(image_files[late_idx])\n        \n        # Display the images\n        plt.figure(figsize=(15, 5))\n        \n        plt.subplot(1, 3, 1)\n        plt.imshow(early_img)\n        plt.title(f'Early Stage (Epoch {(early_idx*SAVE_INTERVAL)+1})')\n        plt.axis('off')\n        \n        plt.subplot(1, 3, 2)\n        plt.imshow(middle_img)\n        plt.title(f'Middle Stage (Epoch {(middle_idx*SAVE_INTERVAL)+1})')\n        plt.axis('off')\n        \n        plt.subplot(1, 3, 3)\n        plt.imshow(late_img)\n        plt.title(f'Late Stage (Epoch {(late_idx*SAVE_INTERVAL)+1})')\n        plt.axis('off')\n        \n        plt.suptitle('Generation Progress Over Training', fontsize=16)\n        plt.tight_layout()\n        plt.savefig('generated_images/training_stages_comparison.png', dpi=300, bbox_inches='tight')\n        plt.show()\n    else:\n        print(\"Not enough saved images found for comparison.\")\nexcept Exception as e:\n    print(f\"Could not load or display images: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T15:07:19.488155Z","iopub.execute_input":"2025-08-01T15:07:19.488406Z","iopub.status.idle":"2025-08-01T15:07:22.202408Z","shell.execute_reply.started":"2025-08-01T15:07:19.488389Z","shell.execute_reply":"2025-08-01T15:07:22.201592Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 13. Conclusion\n\nIn this notebook, we've implemented a GAN to generate realistic fashion images. The training process demonstrated how the generator learns to create increasingly realistic images over time, while the discriminator learns to distinguish between real and fake images.\n\n### Key Achievements:\n- Built a GAN architecture with generator and discriminator networks\n- Implemented adversarial training with appropriate loss functions\n- Visualized the training progress through generated images\n- Saved models and generated images at different training stages\n\n### Next Steps:\n- Try different datasets like CIFAR-10 or CelebA for color images\n- Experiment with more complex GAN architectures like DCGAN, WGAN, or StyleGAN\n- Add conditional inputs to control the type of images generated\n- Improve image quality with techniques like progressive growing or spectral normalization","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}